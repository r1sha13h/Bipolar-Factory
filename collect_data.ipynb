{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Facebook Scraper Library of python language\n",
    "to scrape data from popular news channel's pages\n",
    "on facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facebook_scraper import get_posts as get\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "A Few famous News Channels are : \n",
    "BBC news\n",
    "CNN\n",
    "New York Times\n",
    "Fox News\n",
    "ABC News\n",
    "Huffington Post\n",
    "NBC News\n",
    "Time\n",
    "Daily Mail\n",
    "Business Insider\n",
    "Now This News\n",
    "Al Jazeera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = ['bbcnews','cnn','nytimes',\n",
    "        'FoxNews','ABCNews',\n",
    "        'HuffPostIndia','HuffPostUK',\n",
    "        'NBCNews','time','DailyMail',\n",
    "        'businessinsider','NowThisNews','aljazeera']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will scrape out all the posts present in first 250 feeds of a News page, Time Taken to scrape is mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Date : 2020-04-28 17:22:02.209437\n",
      "bbcnews\n",
      "cnn\n",
      "nytimes\n",
      "FoxNews\n",
      "ABCNews\n",
      "HuffPostIndia\n",
      "HuffPostUK\n",
      "NBCNews\n",
      "time\n",
      "DailyMail\n",
      "businessinsider\n",
      "NowThisNews\n",
      "aljazeera\n",
      "Time taken to scrape data : 45.05 minutes\n"
     ]
    }
   ],
   "source": [
    "now = dt.now()\n",
    "print('Start Date : '+str(now))\n",
    "posts = []\n",
    "for channel in news :     \n",
    "    print(channel)\n",
    "    for post in get(channel,pages = 250) :\n",
    "        post['channel'] = channel\n",
    "        posts.append(post)\n",
    "\n",
    "print('Time taken to scrape data : '+str((dt.now() - now).seconds/60)+' minutes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Copy Created.\n"
     ]
    }
   ],
   "source": [
    "original = posts.copy()\n",
    "print('Source Copy Created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Filtering Out Posts</b> : The pages upload few posts that are irrelavant and hence need to be ignored by us so that it doesnt effect efficiency of our model.Posts having absent <b>time_posted,shares,likes,shared_text,urls</b> are dropped from our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6803 posts refined out of 11100 posts\n"
     ]
    }
   ],
   "source": [
    "posts = original.copy()\n",
    "refined_post = []\n",
    "for post in posts : \n",
    "    \n",
    "    check = post['post_url'] is None or post['post_id'] is None\n",
    "    check = check or post['time'] is None or post['shares'] == 0\n",
    "    check = check or post['likes'] == 0 or len(post['shared_text']) == 0\n",
    "    \n",
    "    if not check : \n",
    "        refined_post.append(post)\n",
    "        \n",
    "posts = refined_post.copy()\n",
    "\n",
    "print(str(len(posts))+' posts refined out of '+str(len(original))+' posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Post\n",
      "post_id : 10157728122492217\n",
      "text : A pregnant nurse who\n",
      "post_text : A pregnant nurse who\n",
      "shared_text : BBC.COM\n",
      "Remembering \n",
      "time : 2020-04-28 16:23:16\n",
      "image : https://external.fbo\n",
      "likes : 2327\n",
      "comments : 333\n",
      "shares : 1015\n",
      "post_url : https://m.facebook.c\n",
      "link : https://bbc.in/2W3Q7\n",
      "channel : bbcnews\n"
     ]
    }
   ],
   "source": [
    "print('Example Post')\n",
    "for i in posts[0] : \n",
    "    string = str(posts[0][i])\n",
    "    if i != 'time' : \n",
    "        string = string[:20]\n",
    "        \n",
    "    print(str(i)+' : '+string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Above dataset we select <b> post_id,shared_text,time (duration between time posted and time scraped),image(present or not - 0/1),likes,comments,shares,post_url,channel </b> name to create a table database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Data\n",
      "   channel                 id  \\\n",
      "0  bbcnews  10157728122492217   \n",
      "1  bbcnews  10157728074267217   \n",
      "2  bbcnews  10157727901822217   \n",
      "3  bbcnews  10157727795172217   \n",
      "4  bbcnews  10157727671812217   \n",
      "\n",
      "                                                 url  timespan  \\\n",
      "0  https://m.facebook.com/story.php?story_fbid=10...  1.891389   \n",
      "1  https://m.facebook.com/story.php?story_fbid=10...  2.320278   \n",
      "2  https://m.facebook.com/story.php?story_fbid=10...  3.813333   \n",
      "3  https://m.facebook.com/story.php?story_fbid=10...  4.589444   \n",
      "4  https://m.facebook.com/story.php?story_fbid=10...  5.345833   \n",
      "\n",
      "                                                text  image   likes  comments  \\\n",
      "0          Remembering 100 NHS workers who have died    1.0  2327.0     333.0   \n",
      "1  Rising virus care home toll leads to record de...    1.0   950.0     197.0   \n",
      "2      Top NYC coronavirus doctor takes her own life    1.0  5854.0    1533.0   \n",
      "3      How NZ beat the virus and got its coffee back    1.0  3488.0     440.0   \n",
      "4  UK to hold minute's silence for fallen key wor...    1.0  4552.0     524.0   \n",
      "\n",
      "   shares  \n",
      "0  1015.0  \n",
      "1   260.0  \n",
      "2  3506.0  \n",
      "3   473.0  \n",
      "4  1209.0  \n"
     ]
    }
   ],
   "source": [
    "col = ['channel','id','url','timespan','text','image','likes','comments','shares']\n",
    "table = pd.DataFrame(columns = col)\n",
    "for post in posts : \n",
    "    index = post['shared_text'].find('\\n')\n",
    "    text = post['shared_text'][index+1 : ]\n",
    "    if len(text) > 0 : \n",
    "        data = [{col[0] : post['channel'], col[1] : post['post_id'],\n",
    "                 col[2] : post['post_url'],col[3] : (datetime.datetime.now() - post['time']).seconds/3600,\n",
    "                 col[4] : text,\n",
    "                 col[5] : np.float64(post['image'] is not None),col[6] : np.float64(post['likes']),\n",
    "                 col[7] : np.float64(post['comments']),col[8] : np.float64(post['shares'])}]\n",
    "        \n",
    "        table = table.append(data,ignore_index=True,sort=False)\n",
    "\n",
    "print('Table Data')\n",
    "print(table.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we save the table as csv file for <b> Later use </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table saved as .csv format\n"
     ]
    }
   ],
   "source": [
    "table_copy = table.copy()\n",
    "table.to_csv('News.csv',index = False)\n",
    "print('Table saved as .csv format')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
